{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Spark \u7b14\u8bb0 \u00b6 Spark \u5b66\u4e60\u7b14\u8bb0 Reference \u00b6 Spark Overview","title":"\u9996\u9875"},{"location":"#spark","text":"Spark \u5b66\u4e60\u7b14\u8bb0","title":"Spark \u7b14\u8bb0"},{"location":"#reference","text":"Spark Overview","title":"Reference"},{"location":"by/","text":"By \u00b6 CREATE TABLE \u00b6 PARTITIONED BY \u00b6 [ PARTITIONED BY ( col3 data_type [ COMMENT col_comment ], ...)] CLUSTERED/SORTED BY \u00b6 [ CLUSTERED BY ( col1 , ...) [ SORTED BY ( col1 [ ASC | DESC ], ...)] INTO num_buckets BUCKETS ] SKEWED BY \u00b6 [ SKEWED BY ( col1 , col2 , ...) ON (( col_value , col_value , ...), ...) Query \u00b6 ORDER BY \u00b6 CLUSTER BY \u00b6 DISTRIBUTE BY(repartition) \u00b6 # The default number of partitions to use when shuffling data for joins or aggregations. spark.sql.shuffle.partitions = 200 SORT BY \u00b6 WINDOW \u00b6 CLUSTER BY \u00b6 PARTITION|DISTRIBUTE BY \u00b6 ORDER|SORT BY \u00b6 Writer \u00b6 partitionBy \u00b6 bucketBy \u00b6 sortBy \u00b6 Reference \u00b6","title":"By"},{"location":"by/#by","text":"","title":"By"},{"location":"by/#create-table","text":"","title":"CREATE TABLE"},{"location":"by/#partitioned-by","text":"[ PARTITIONED BY ( col3 data_type [ COMMENT col_comment ], ...)]","title":"PARTITIONED BY"},{"location":"by/#clusteredsorted-by","text":"[ CLUSTERED BY ( col1 , ...) [ SORTED BY ( col1 [ ASC | DESC ], ...)] INTO num_buckets BUCKETS ]","title":"CLUSTERED/SORTED BY"},{"location":"by/#skewed-by","text":"[ SKEWED BY ( col1 , col2 , ...) ON (( col_value , col_value , ...), ...)","title":"SKEWED BY"},{"location":"by/#query","text":"","title":"Query"},{"location":"by/#order-by","text":"","title":"ORDER BY"},{"location":"by/#cluster-by","text":"","title":"CLUSTER BY"},{"location":"by/#distribute-byrepartition","text":"# The default number of partitions to use when shuffling data for joins or aggregations. spark.sql.shuffle.partitions = 200","title":"DISTRIBUTE BY(repartition)"},{"location":"by/#sort-by","text":"","title":"SORT BY"},{"location":"by/#window","text":"","title":"WINDOW"},{"location":"by/#cluster-by_1","text":"","title":"CLUSTER BY"},{"location":"by/#partitiondistribute-by","text":"","title":"PARTITION|DISTRIBUTE BY"},{"location":"by/#ordersort-by","text":"","title":"ORDER|SORT BY"},{"location":"by/#writer","text":"","title":"Writer"},{"location":"by/#partitionby","text":"","title":"partitionBy"},{"location":"by/#bucketby","text":"","title":"bucketBy"},{"location":"by/#sortby","text":"","title":"sortBy"},{"location":"by/#reference","text":"","title":"Reference"},{"location":"bykey/","text":"ByKey \u00b6 RDD \u00b6 JavaPairRDD \u00b6 sampleByKey/sampleByKeyExact \u00b6 \u901a\u8fc7 Key \u8fd4\u56de\u5bf9 RDD \u5206\u5c42\u62bd\u6837\u7684\u5b50\u96c6\uff0cmath.ceil(numItems * samplingRate)\uff0c\u4ee5\u4e0a\u4e3a\u8fd1\u4f3c\u548c\u7cbe\u786e\u4e24\u79cd\u7c7b\u578b\uff0c\u5176\u4e2d\uff0c\u7cbe\u786e\u65e0\u653e\u56de\u9700\u8981\u904d\u5386\u4e00\u6b21\u6570\u636e\uff0c\u7cbe\u786e\u6709\u653e\u56de\u9700\u8981\u904d\u5386\u4e24\u6b21\u6570\u636e Spark\u5165\u95e8\uff1a\u57fa\u672c\u7684\u7edf\u8ba1\u5de5\u5177\uff081\uff09 \u2013 spark.mllib \u53f2\u4e0a\u6700\u5168\u91c7\u6837\u65b9\u6cd5\u8be6\u7ec6\u89e3\u8bfb\u4e0e\u4ee3\u7801\u5b9e\u73b0 combineByKey/aggregateByKey/foldByKey/reduceByKey \u00b6 Map \u7aef\uff1amapSideCombine=true \u65f6\u751f\u6548\uff0c\u6309\u7167 key \u7684\u4e0d\u540c\uff0c\u5c06 createCombiner \u51fd\u6570\u5e94\u7528\u4e8e\u8be5 key \u5728\u8be5 partition \u5185\u7684\u7b2c\u4e00\u6761\u8bb0\u5f55\uff0c\u5e76\u4e14\u5bf9\u4e8e\u5728\u8be5 partition \u4e2d\u76f8\u540c key \u7684\u8bb0\u5f55\uff0c\u5e94\u7528 mergeValue \u5904\u7406\u8bfb\u5165\u8bb0\u5f55\uff0c\u5e76\u5f97\u5230 C \u7c7b\u578b\u7684\u503c\uff0c\u6700\u7ec8 map \u7aef\u7684\u6bcf\u4e2a\u5206\u533a\u5f97\u5230\u4e00\u7cfb\u5217 RDD[(K, C)] \u7c7b\u578b\u7684\u5bf9\u8c61\uff08map \u7aef\u4e0d\u540cpartition \u8f93\u51fa\u53ef\u80fd\u5177\u6709\u76f8\u540c\u7684 key\uff09 Reduce \u7aef\uff1a\u6839\u636e partitioner \u8fdb\u884c shuffle\uff0c\u540c\u4e00\u4e2a key \u6240\u6709 map \u8f93\u51fa\u7ed3\u679c\u5b58\u653e\u5230\u540c\u4e00\u4e2a partition \u5185\uff0c\u5c06\u76f8\u540c key \u7684\u6570\u636e\u5e94\u7528 mergeCombiners\uff0c\u6700\u7ec8\u5f97\u5230 RDD[(K, C)] \u7c7b\u578b\u7684\u8f93\u51fa\u7ed3\u679c groupByKey\uff1amapSideCombine = false\uff0cC = V\uff0c\u4e00\u822c\u573a\u666f reduceByKey\uff1amapSideCombine = true\uff0cC = V\uff0c\u4e00\u822c\u573a\u666f\uff0c\u76f8\u6bd4 groupByKey\uff0c\u5728 Map \u7aef\u805a\u5408\u4f18\u5316 foldByKey\uff1amapSideCombine = true\uff0czeroValue\uff0cC = V\uff0c\u9700\u8981\u521d\u59cb\u503c\u573a\u666f aggregateByKey\uff1amapSideCombine = true\uff0czeroValue\uff0c\u9700\u8981\u521d\u59cb\u503c\uff0c\u8f93\u5165\u8f93\u51fa\u4e0d\u540c\u7c7b\u578b\u573a\u666f combineByKey\uff1a\u6309\u7167\u9700\u8981\u63a7\u5236\u573a\u666f /** * :: Experimental :: * Generic function to combine the elements for each key using a custom set of aggregation * functions. Turns an RDD[(K, V)] into a result of type RDD[(K, C)], for a \"combined type\" C * * Users provide three functions: * * - `createCombiner`, which turns a V into a C (e.g., creates a one-element list) * - `mergeValue`, to merge a V into a C (e.g., adds it to the end of a list) * - `mergeCombiners`, to combine two C's into a single one. * * In addition, users can control the partitioning of the output RDD, and whether to perform * map-side aggregation (if a mapper can produce multiple items with the same key). * * @note V and C can be different -- for example, one might group an RDD of type * (Int, Int) into an RDD of type (Int, Seq[Int]). */ @Experimental def combineByKeyWithClassTag [ C ]( createCombiner : V => C , // \u5c06 V \u7c7b\u578b\u503c\u8f6c\u6362\u6210 C \u7c7b\u578b\u503c mergeValue : ( C , V ) => C , // \u5c06\u4e00\u4e2a V \u7c7b\u578b\u503c\u4e0e\u4e00\u4e2a C \u7c7b\u578b\u503c\u5408\u5e76\u6210 C \u7c7b\u578b\u503c mergeCombiners : ( C , C ) => C , // \u5c06\u4e24\u4e2a C \u7c7b\u578b\u503c\u5408\u5e76\u4e3a\u4e00\u4e2a C \u7c7b\u578b\u503c partitioner : Partitioner , // \u5206\u533a\u51fd\u6570 mapSideCombine : Boolean = true , // \u662f\u5426\u5728 Map \u7aef\u8fdb\u884c combine \u64cd\u4f5c serializer : Serializer = null // \u6307\u5b9a\u5e8f\u5217\u5316\u65b9\u5f0f )( implicit ct : ClassTag [ C ]) : RDD [( K , C )] = self . withScope { require ( mergeCombiners != null , \"mergeCombiners must be defined\" ) // required as of Spark 0.9.0 if ( keyClass . isArray ) { if ( mapSideCombine ) { throw new SparkException ( \"Cannot use map-side combining with array keys.\" ) } if ( partitioner . isInstanceOf [ HashPartitioner ]) { throw new SparkException ( \"HashPartitioner cannot partition array keys.\" ) } } val aggregator = new Aggregator [ K , V , C ]( self . context . clean ( createCombiner ), self . context . clean ( mergeValue ), self . context . clean ( mergeCombiners )) if ( self . partitioner == Some ( partitioner )) { self . mapPartitions ( iter => { val context = TaskContext . get () new InterruptibleIterator ( context , aggregator . combineValuesByKey ( iter , context )) }, preservesPartitioning = true ) } else { new ShuffledRDD [ K , V , C ]( self , partitioner ) . setSerializer ( serializer ) . setAggregator ( aggregator ) . setMapSideCombine ( mapSideCombine ) } } sortByKey \u00b6 /** * Sort the RDD by key, so that each partition contains a sorted range of the elements. Calling * `collect` or `save` on the resulting RDD will return or output an ordered list of records * (in the `save` case, they will be written to multiple `part-X` files in the filesystem, in * order of the keys). */ // TODO: this currently doesn't work on P other than Tuple2! def sortByKey ( ascending : Boolean = true , // \u6392\u5e8f\u65b9\u5f0f numPartitions : Int = self . partitions . length // \u5206\u533a\u6570 ) : RDD [( K , V )] = self . withScope { val part = new RangePartitioner ( numPartitions , self , ascending ) new ShuffledRDD [ K , V , V ]( self , part ) . setKeyOrdering ( if ( ascending ) ordering else ordering . reverse ) } subtractByKey \u00b6 subtractByKey \u5b9e\u73b0\u65b9\u5f0f\u4e3a\uff0c\u5c06 rdd1 \u6570\u636e\u4fdd\u5b58\u5728\u5185\u5b58\u4e2d\uff0c\u901a\u8fc7 rdd2 \u6765\u8fc7\u6ee4\u662f\u5426\u8981\u5220\u9664 rdd1 \u7684\u503c\uff0c\u907f\u514d\u4e86 OOM \u7684\u98ce\u9669\uff1b subtractByKey \u662f cogroup \u7684\u4f18\u5316\u7248\u672c\uff0c cogroup \u5b9e\u73b0\u6548\u7387\u8f83\u4f4e\uff0c\u5c06 rdd1 \u4e2d\u7684\u5339\u914d\u548c\u4e0d\u5339\u914d rdd2 \u7684\u503c\u7684\u6240\u6709\u6761\u76ee\u90fd\u4fdd\u5b58\u5728 JHashMap \u4e2d\uff0c\u76f4\u5230\u5339\u914d\u7ed3\u675f Reference \u00b6","title":"ByKey"},{"location":"bykey/#bykey","text":"","title":"ByKey"},{"location":"bykey/#rdd","text":"","title":"RDD"},{"location":"bykey/#javapairrdd","text":"","title":"JavaPairRDD"},{"location":"bykey/#samplebykeysamplebykeyexact","text":"\u901a\u8fc7 Key \u8fd4\u56de\u5bf9 RDD \u5206\u5c42\u62bd\u6837\u7684\u5b50\u96c6\uff0cmath.ceil(numItems * samplingRate)\uff0c\u4ee5\u4e0a\u4e3a\u8fd1\u4f3c\u548c\u7cbe\u786e\u4e24\u79cd\u7c7b\u578b\uff0c\u5176\u4e2d\uff0c\u7cbe\u786e\u65e0\u653e\u56de\u9700\u8981\u904d\u5386\u4e00\u6b21\u6570\u636e\uff0c\u7cbe\u786e\u6709\u653e\u56de\u9700\u8981\u904d\u5386\u4e24\u6b21\u6570\u636e Spark\u5165\u95e8\uff1a\u57fa\u672c\u7684\u7edf\u8ba1\u5de5\u5177\uff081\uff09 \u2013 spark.mllib \u53f2\u4e0a\u6700\u5168\u91c7\u6837\u65b9\u6cd5\u8be6\u7ec6\u89e3\u8bfb\u4e0e\u4ee3\u7801\u5b9e\u73b0","title":"sampleByKey/sampleByKeyExact"},{"location":"bykey/#combinebykeyaggregatebykeyfoldbykeyreducebykey","text":"Map \u7aef\uff1amapSideCombine=true \u65f6\u751f\u6548\uff0c\u6309\u7167 key \u7684\u4e0d\u540c\uff0c\u5c06 createCombiner \u51fd\u6570\u5e94\u7528\u4e8e\u8be5 key \u5728\u8be5 partition \u5185\u7684\u7b2c\u4e00\u6761\u8bb0\u5f55\uff0c\u5e76\u4e14\u5bf9\u4e8e\u5728\u8be5 partition \u4e2d\u76f8\u540c key \u7684\u8bb0\u5f55\uff0c\u5e94\u7528 mergeValue \u5904\u7406\u8bfb\u5165\u8bb0\u5f55\uff0c\u5e76\u5f97\u5230 C \u7c7b\u578b\u7684\u503c\uff0c\u6700\u7ec8 map \u7aef\u7684\u6bcf\u4e2a\u5206\u533a\u5f97\u5230\u4e00\u7cfb\u5217 RDD[(K, C)] \u7c7b\u578b\u7684\u5bf9\u8c61\uff08map \u7aef\u4e0d\u540cpartition \u8f93\u51fa\u53ef\u80fd\u5177\u6709\u76f8\u540c\u7684 key\uff09 Reduce \u7aef\uff1a\u6839\u636e partitioner \u8fdb\u884c shuffle\uff0c\u540c\u4e00\u4e2a key \u6240\u6709 map \u8f93\u51fa\u7ed3\u679c\u5b58\u653e\u5230\u540c\u4e00\u4e2a partition \u5185\uff0c\u5c06\u76f8\u540c key \u7684\u6570\u636e\u5e94\u7528 mergeCombiners\uff0c\u6700\u7ec8\u5f97\u5230 RDD[(K, C)] \u7c7b\u578b\u7684\u8f93\u51fa\u7ed3\u679c groupByKey\uff1amapSideCombine = false\uff0cC = V\uff0c\u4e00\u822c\u573a\u666f reduceByKey\uff1amapSideCombine = true\uff0cC = V\uff0c\u4e00\u822c\u573a\u666f\uff0c\u76f8\u6bd4 groupByKey\uff0c\u5728 Map \u7aef\u805a\u5408\u4f18\u5316 foldByKey\uff1amapSideCombine = true\uff0czeroValue\uff0cC = V\uff0c\u9700\u8981\u521d\u59cb\u503c\u573a\u666f aggregateByKey\uff1amapSideCombine = true\uff0czeroValue\uff0c\u9700\u8981\u521d\u59cb\u503c\uff0c\u8f93\u5165\u8f93\u51fa\u4e0d\u540c\u7c7b\u578b\u573a\u666f combineByKey\uff1a\u6309\u7167\u9700\u8981\u63a7\u5236\u573a\u666f /** * :: Experimental :: * Generic function to combine the elements for each key using a custom set of aggregation * functions. Turns an RDD[(K, V)] into a result of type RDD[(K, C)], for a \"combined type\" C * * Users provide three functions: * * - `createCombiner`, which turns a V into a C (e.g., creates a one-element list) * - `mergeValue`, to merge a V into a C (e.g., adds it to the end of a list) * - `mergeCombiners`, to combine two C's into a single one. * * In addition, users can control the partitioning of the output RDD, and whether to perform * map-side aggregation (if a mapper can produce multiple items with the same key). * * @note V and C can be different -- for example, one might group an RDD of type * (Int, Int) into an RDD of type (Int, Seq[Int]). */ @Experimental def combineByKeyWithClassTag [ C ]( createCombiner : V => C , // \u5c06 V \u7c7b\u578b\u503c\u8f6c\u6362\u6210 C \u7c7b\u578b\u503c mergeValue : ( C , V ) => C , // \u5c06\u4e00\u4e2a V \u7c7b\u578b\u503c\u4e0e\u4e00\u4e2a C \u7c7b\u578b\u503c\u5408\u5e76\u6210 C \u7c7b\u578b\u503c mergeCombiners : ( C , C ) => C , // \u5c06\u4e24\u4e2a C \u7c7b\u578b\u503c\u5408\u5e76\u4e3a\u4e00\u4e2a C \u7c7b\u578b\u503c partitioner : Partitioner , // \u5206\u533a\u51fd\u6570 mapSideCombine : Boolean = true , // \u662f\u5426\u5728 Map \u7aef\u8fdb\u884c combine \u64cd\u4f5c serializer : Serializer = null // \u6307\u5b9a\u5e8f\u5217\u5316\u65b9\u5f0f )( implicit ct : ClassTag [ C ]) : RDD [( K , C )] = self . withScope { require ( mergeCombiners != null , \"mergeCombiners must be defined\" ) // required as of Spark 0.9.0 if ( keyClass . isArray ) { if ( mapSideCombine ) { throw new SparkException ( \"Cannot use map-side combining with array keys.\" ) } if ( partitioner . isInstanceOf [ HashPartitioner ]) { throw new SparkException ( \"HashPartitioner cannot partition array keys.\" ) } } val aggregator = new Aggregator [ K , V , C ]( self . context . clean ( createCombiner ), self . context . clean ( mergeValue ), self . context . clean ( mergeCombiners )) if ( self . partitioner == Some ( partitioner )) { self . mapPartitions ( iter => { val context = TaskContext . get () new InterruptibleIterator ( context , aggregator . combineValuesByKey ( iter , context )) }, preservesPartitioning = true ) } else { new ShuffledRDD [ K , V , C ]( self , partitioner ) . setSerializer ( serializer ) . setAggregator ( aggregator ) . setMapSideCombine ( mapSideCombine ) } }","title":"combineByKey/aggregateByKey/foldByKey/reduceByKey"},{"location":"bykey/#sortbykey","text":"/** * Sort the RDD by key, so that each partition contains a sorted range of the elements. Calling * `collect` or `save` on the resulting RDD will return or output an ordered list of records * (in the `save` case, they will be written to multiple `part-X` files in the filesystem, in * order of the keys). */ // TODO: this currently doesn't work on P other than Tuple2! def sortByKey ( ascending : Boolean = true , // \u6392\u5e8f\u65b9\u5f0f numPartitions : Int = self . partitions . length // \u5206\u533a\u6570 ) : RDD [( K , V )] = self . withScope { val part = new RangePartitioner ( numPartitions , self , ascending ) new ShuffledRDD [ K , V , V ]( self , part ) . setKeyOrdering ( if ( ascending ) ordering else ordering . reverse ) }","title":"sortByKey"},{"location":"bykey/#subtractbykey","text":"subtractByKey \u5b9e\u73b0\u65b9\u5f0f\u4e3a\uff0c\u5c06 rdd1 \u6570\u636e\u4fdd\u5b58\u5728\u5185\u5b58\u4e2d\uff0c\u901a\u8fc7 rdd2 \u6765\u8fc7\u6ee4\u662f\u5426\u8981\u5220\u9664 rdd1 \u7684\u503c\uff0c\u907f\u514d\u4e86 OOM \u7684\u98ce\u9669\uff1b subtractByKey \u662f cogroup \u7684\u4f18\u5316\u7248\u672c\uff0c cogroup \u5b9e\u73b0\u6548\u7387\u8f83\u4f4e\uff0c\u5c06 rdd1 \u4e2d\u7684\u5339\u914d\u548c\u4e0d\u5339\u914d rdd2 \u7684\u503c\u7684\u6240\u6709\u6761\u76ee\u90fd\u4fdd\u5b58\u5728 JHashMap \u4e2d\uff0c\u76f4\u5230\u5339\u914d\u7ed3\u675f","title":"subtractByKey"},{"location":"bykey/#reference","text":"","title":"Reference"},{"location":"execution/","text":"Execution \u00b6 Spark Execution Memory \u00b6 Task \u00b6 Task\u662fSpark\u4f5c\u4e1a\u8fd0\u884c\u6700\u5c0f\u5355\u5143\u3002 TaskContext -> TaskContextImpl ShuffleBlockResolver -> IndexShuffleBlockResolver ShuffleMapTask ResultTask Reference \u00b6","title":"Execution"},{"location":"execution/#execution","text":"","title":"Execution"},{"location":"execution/#spark-execution-memory","text":"","title":"Spark Execution Memory"},{"location":"execution/#task","text":"Task\u662fSpark\u4f5c\u4e1a\u8fd0\u884c\u6700\u5c0f\u5355\u5143\u3002 TaskContext -> TaskContextImpl ShuffleBlockResolver -> IndexShuffleBlockResolver ShuffleMapTask ResultTask","title":"Task"},{"location":"execution/#reference","text":"","title":"Reference"},{"location":"graphx/","text":"GraphX \u00b6 Graph \u00b6 Vertex[VertexRDD]\u3001Edge[EdgeRDD]\u3001Triplet[EdgeTriplet]\u3001RoutingTable PartitionStrategy \u00b6 RandomVertexCut CanonicalRandomVertexCut EdgePartition1D EdgePartition2D Construct \u00b6 apply fromEdges fromEdgeTuples Functionality \u00b6 /** Summary of the functionality in the property graph */ class Graph [ VD , ED ] { // Information about the Graph =================================================================== val numEdges : Long val numVertices : Long val inDegrees : VertexRDD [ Int ] val outDegrees : VertexRDD [ Int ] val degrees : VertexRDD [ Int ] // Views of the graph as collections ============================================================= val vertices : VertexRDD [ VD ] val edges : EdgeRDD [ ED ] val triplets : RDD [ EdgeTriplet [ VD , ED ]] // Functions for caching graphs ================================================================== def persist ( newLevel : StorageLevel = StorageLevel . MEMORY_ONLY ) : Graph [ VD , ED ] def cache () : Graph [ VD , ED ] def unpersistVertices ( blocking : Boolean = true ) : Graph [ VD , ED ] // Change the partitioning heuristic ============================================================ def partitionBy ( partitionStrategy : PartitionStrategy ) : Graph [ VD , ED ] // Transform vertex and edge attributes ========================================================== def mapVertices [ VD2 ]( map : ( VertexId , VD ) => VD2 ) : Graph [ VD2 , ED ] def mapEdges [ ED2 ]( map : Edge [ ED ] => ED2 ) : Graph [ VD , ED2 ] def mapEdges [ ED2 ]( map : ( PartitionID , Iterator [ Edge [ ED ]]) => Iterator [ ED2 ]) : Graph [ VD , ED2 ] def mapTriplets [ ED2 ]( map : EdgeTriplet [ VD , ED ] => ED2 ) : Graph [ VD , ED2 ] def mapTriplets [ ED2 ]( map : ( PartitionID , Iterator [ EdgeTriplet [ VD , ED ]]) => Iterator [ ED2 ]) : Graph [ VD , ED2 ] // Modify the graph structure ==================================================================== def reverse : Graph [ VD , ED ] def subgraph ( epred : EdgeTriplet [ VD , ED ] => Boolean = ( x => true ), vpred : ( VertexId , VD ) => Boolean = (( v , d ) => true )) : Graph [ VD , ED ] def mask [ VD2 , ED2 ]( other : Graph [ VD2 , ED2 ]) : Graph [ VD , ED ] def groupEdges ( merge : ( ED , ED ) => ED ) : Graph [ VD , ED ] // Join RDDs with the graph ====================================================================== def joinVertices [ U ]( table : RDD [( VertexId , U )])( mapFunc : ( VertexId , VD , U ) => VD ) : Graph [ VD , ED ] def outerJoinVertices [ U , VD2 ]( other : RDD [( VertexId , U )]) ( mapFunc : ( VertexId , VD , Option [ U ]) => VD2 ) : Graph [ VD2 , ED ] // Aggregate information about adjacent triplets ================================================= def collectNeighborIds ( edgeDirection : EdgeDirection ) : VertexRDD [ Array [ VertexId ]] def collectNeighbors ( edgeDirection : EdgeDirection ) : VertexRDD [ Array [( VertexId , VD )]] def aggregateMessages [ Msg: ClassTag ]( sendMsg : EdgeContext [ VD , ED , Msg ] => Unit , mergeMsg : ( Msg , Msg ) => Msg , tripletFields : TripletFields = TripletFields . All ) : VertexRDD [ A ] // Iterative graph-parallel computation ========================================================== def pregel [ A ]( initialMsg : A , maxIterations : Int , activeDirection : EdgeDirection )( vprog : ( VertexId , VD , A ) => VD , sendMsg : EdgeTriplet [ VD , ED ] => Iterator [( VertexId , A )], mergeMsg : ( A , A ) => A ) : Graph [ VD , ED ] // Basic graph algorithms ======================================================================== def pageRank ( tol : Double , resetProb : Double = 0.15 ) : Graph [ Double , Double ] def connectedComponents () : Graph [ VertexId , ED ] def triangleCount () : Graph [ Int , ED ] def stronglyConnectedComponents ( numIter : Int ) : Graph [ VertexId , ED ] } Algorithm \u00b6 Connected Components Label Propagation PageRank Shortest Paths Strongly Connected Components SVD++ Triangle Count Reference \u00b6 GraphX Programming Guide GraphFrames Overview GraphFrames and GraphX Spark GraphX Source Analysis","title":"GraphX"},{"location":"graphx/#graphx","text":"","title":"GraphX"},{"location":"graphx/#graph","text":"Vertex[VertexRDD]\u3001Edge[EdgeRDD]\u3001Triplet[EdgeTriplet]\u3001RoutingTable","title":"Graph"},{"location":"graphx/#partitionstrategy","text":"RandomVertexCut CanonicalRandomVertexCut EdgePartition1D EdgePartition2D","title":"PartitionStrategy"},{"location":"graphx/#construct","text":"apply fromEdges fromEdgeTuples","title":"Construct"},{"location":"graphx/#functionality","text":"/** Summary of the functionality in the property graph */ class Graph [ VD , ED ] { // Information about the Graph =================================================================== val numEdges : Long val numVertices : Long val inDegrees : VertexRDD [ Int ] val outDegrees : VertexRDD [ Int ] val degrees : VertexRDD [ Int ] // Views of the graph as collections ============================================================= val vertices : VertexRDD [ VD ] val edges : EdgeRDD [ ED ] val triplets : RDD [ EdgeTriplet [ VD , ED ]] // Functions for caching graphs ================================================================== def persist ( newLevel : StorageLevel = StorageLevel . MEMORY_ONLY ) : Graph [ VD , ED ] def cache () : Graph [ VD , ED ] def unpersistVertices ( blocking : Boolean = true ) : Graph [ VD , ED ] // Change the partitioning heuristic ============================================================ def partitionBy ( partitionStrategy : PartitionStrategy ) : Graph [ VD , ED ] // Transform vertex and edge attributes ========================================================== def mapVertices [ VD2 ]( map : ( VertexId , VD ) => VD2 ) : Graph [ VD2 , ED ] def mapEdges [ ED2 ]( map : Edge [ ED ] => ED2 ) : Graph [ VD , ED2 ] def mapEdges [ ED2 ]( map : ( PartitionID , Iterator [ Edge [ ED ]]) => Iterator [ ED2 ]) : Graph [ VD , ED2 ] def mapTriplets [ ED2 ]( map : EdgeTriplet [ VD , ED ] => ED2 ) : Graph [ VD , ED2 ] def mapTriplets [ ED2 ]( map : ( PartitionID , Iterator [ EdgeTriplet [ VD , ED ]]) => Iterator [ ED2 ]) : Graph [ VD , ED2 ] // Modify the graph structure ==================================================================== def reverse : Graph [ VD , ED ] def subgraph ( epred : EdgeTriplet [ VD , ED ] => Boolean = ( x => true ), vpred : ( VertexId , VD ) => Boolean = (( v , d ) => true )) : Graph [ VD , ED ] def mask [ VD2 , ED2 ]( other : Graph [ VD2 , ED2 ]) : Graph [ VD , ED ] def groupEdges ( merge : ( ED , ED ) => ED ) : Graph [ VD , ED ] // Join RDDs with the graph ====================================================================== def joinVertices [ U ]( table : RDD [( VertexId , U )])( mapFunc : ( VertexId , VD , U ) => VD ) : Graph [ VD , ED ] def outerJoinVertices [ U , VD2 ]( other : RDD [( VertexId , U )]) ( mapFunc : ( VertexId , VD , Option [ U ]) => VD2 ) : Graph [ VD2 , ED ] // Aggregate information about adjacent triplets ================================================= def collectNeighborIds ( edgeDirection : EdgeDirection ) : VertexRDD [ Array [ VertexId ]] def collectNeighbors ( edgeDirection : EdgeDirection ) : VertexRDD [ Array [( VertexId , VD )]] def aggregateMessages [ Msg: ClassTag ]( sendMsg : EdgeContext [ VD , ED , Msg ] => Unit , mergeMsg : ( Msg , Msg ) => Msg , tripletFields : TripletFields = TripletFields . All ) : VertexRDD [ A ] // Iterative graph-parallel computation ========================================================== def pregel [ A ]( initialMsg : A , maxIterations : Int , activeDirection : EdgeDirection )( vprog : ( VertexId , VD , A ) => VD , sendMsg : EdgeTriplet [ VD , ED ] => Iterator [( VertexId , A )], mergeMsg : ( A , A ) => A ) : Graph [ VD , ED ] // Basic graph algorithms ======================================================================== def pageRank ( tol : Double , resetProb : Double = 0.15 ) : Graph [ Double , Double ] def connectedComponents () : Graph [ VertexId , ED ] def triangleCount () : Graph [ Int , ED ] def stronglyConnectedComponents ( numIter : Int ) : Graph [ VertexId , ED ] }","title":"Functionality"},{"location":"graphx/#algorithm","text":"Connected Components Label Propagation PageRank Shortest Paths Strongly Connected Components SVD++ Triangle Count","title":"Algorithm"},{"location":"graphx/#reference","text":"GraphX Programming Guide GraphFrames Overview GraphFrames and GraphX Spark GraphX Source Analysis","title":"Reference"},{"location":"hint/","text":"Hint \u00b6 hint : '/*+' hintStatements+=hintStatement (','? hintStatements+=hintStatement)* '*/' ; hintStatement : hintName=identifier | hintName=identifier '(' parameters+=primaryExpression (',' parameters+=primaryExpression)* ')' ; HINT SELECT /*+ HINT(t) */ * FROM t BROADCASTJOIN SELECT /*+ BROADCASTJOIN(b) */ * FROM T1 a JOIN T2 b ON a . key = b . key MAPJOIN SELECT /*+ MAPJOIN(b) */ * FROM T1 a JOIN T2 b ON a . key = b . key STREAMTABLE SELECT /*+ STREAMTABLE(b) */ * FROM T1 a JOIN T2 b ON a . key = b . key INDEX SELECT /*+ INDEX(t, ix_job_name) */ * FROM t COALESCE SELECT /*+ COALESCE(200) */ * FROM t REPARTITION SELECT /*+ REPARTITION(200) */ * FROM t Reference \u00b6","title":"Hint"},{"location":"hint/#hint","text":"hint : '/*+' hintStatements+=hintStatement (','? hintStatements+=hintStatement)* '*/' ; hintStatement : hintName=identifier | hintName=identifier '(' parameters+=primaryExpression (',' parameters+=primaryExpression)* ')' ; HINT SELECT /*+ HINT(t) */ * FROM t BROADCASTJOIN SELECT /*+ BROADCASTJOIN(b) */ * FROM T1 a JOIN T2 b ON a . key = b . key MAPJOIN SELECT /*+ MAPJOIN(b) */ * FROM T1 a JOIN T2 b ON a . key = b . key STREAMTABLE SELECT /*+ STREAMTABLE(b) */ * FROM T1 a JOIN T2 b ON a . key = b . key INDEX SELECT /*+ INDEX(t, ix_job_name) */ * FROM t COALESCE SELECT /*+ COALESCE(200) */ * FROM t REPARTITION SELECT /*+ REPARTITION(200) */ * FROM t","title":"Hint"},{"location":"hint/#reference","text":"","title":"Reference"},{"location":"join/","text":"Join \u00b6 joinRelation : (joinType) JOIN right=relationPrimary joinCriteria? | NATURAL joinType JOIN right=relationPrimary ; joinType : INNER? | CROSS | LEFT OUTER? | LEFT SEMI | RIGHT OUTER? | FULL OUTER? | LEFT? ANTI ; joinCriteria : ON booleanExpression | USING identifierList ; /** * Select the proper physical plan for join based on join strategy hints, the availability of * equi-join keys and the sizes of joining relations. Below are the existing join strategies, * their characteristics and their limitations. * * - Broadcast hash join (BHJ): * Only supported for equi-joins, while the join keys do not need to be sortable. * Supported for all join types except full outer joins. * BHJ usually performs faster than the other join algorithms when the broadcast side is * small. However, broadcasting tables is a network-intensive operation and it could cause * OOM or perform badly in some cases, especially when the build/broadcast side is big. * * - Shuffle hash join: * Only supported for equi-joins, while the join keys do not need to be sortable. * Supported for all join types except full outer joins. * * - Shuffle sort merge join (SMJ): * Only supported for equi-joins and the join keys have to be sortable. * Supported for all join types. * * - Broadcast nested loop join (BNLJ): * Supports both equi-joins and non-equi-joins. * Supports all the join types, but the implementation is optimized for: * 1) broadcasting the left side in a right outer join; * 2) broadcasting the right side in a left outer, left semi, left anti or existence join; * 3) broadcasting either side in an inner-like join. * For other cases, we need to scan the data multiple times, which can be rather slow. * * - Shuffle-and-replicate nested loop join (a.k.a. cartesian product join): * Supports both equi-joins and non-equi-joins. * Supports only inner like joins. */ object JoinSelection extends Strategy with PredicateHelper { /** * Matches a plan whose output should be small enough to be used in broadcast join. */ private def canBroadcast ( plan : LogicalPlan ) : Boolean = { plan . stats . sizeInBytes >= 0 && plan . stats . sizeInBytes <= conf . autoBroadcastJoinThreshold } /** * Matches a plan whose single partition should be small enough to build a hash table. * * Note: this assume that the number of partition is fixed, requires additional work if it's * dynamic. */ private def canBuildLocalHashMap ( plan : LogicalPlan ) : Boolean = { plan . stats . sizeInBytes < conf . autoBroadcastJoinThreshold * conf . numShufflePartitions } /** * Returns whether plan a is much smaller (3X) than plan b. * * The cost to build hash map is higher than sorting, we should only build hash map on a table * that is much smaller than other one. Since we does not have the statistic for number of rows, * use the size of bytes here as estimation. */ private def muchSmaller ( a : LogicalPlan , b : LogicalPlan ) : Boolean = { a . stats . sizeInBytes * 3 <= b . stats . sizeInBytes } private def canBuildRight ( joinType : JoinType ) : Boolean = joinType match { case _: InnerLike | LeftOuter | LeftSemi | LeftAnti | _ : ExistenceJoin => true case _ => false } private def canBuildLeft ( joinType : JoinType ) : Boolean = joinType match { case _: InnerLike | RightOuter => true case _ => false } private def getBuildSide ( wantToBuildLeft : Boolean , wantToBuildRight : Boolean , left : LogicalPlan , right : LogicalPlan ) : Option [ BuildSide ] = { if ( wantToBuildLeft && wantToBuildRight ) { // returns the smaller side base on its estimated physical size, if we want to build the // both sides. Some ( getSmallerSide ( left , right )) } else if ( wantToBuildLeft ) { Some ( BuildLeft ) } else if ( wantToBuildRight ) { Some ( BuildRight ) } else { None } } private def getSmallerSide ( left : LogicalPlan , right : LogicalPlan ) = { if ( right . stats . sizeInBytes <= left . stats . sizeInBytes ) BuildRight else BuildLeft } private def hintToBroadcastLeft ( hint : JoinHint ) : Boolean = { hint . leftHint . exists ( _ . strategy . contains ( BROADCAST )) } private def hintToBroadcastRight ( hint : JoinHint ) : Boolean = { hint . rightHint . exists ( _ . strategy . contains ( BROADCAST )) } private def hintToShuffleHashLeft ( hint : JoinHint ) : Boolean = { hint . leftHint . exists ( _ . strategy . contains ( SHUFFLE_HASH )) } private def hintToShuffleHashRight ( hint : JoinHint ) : Boolean = { hint . rightHint . exists ( _ . strategy . contains ( SHUFFLE_HASH )) } private def hintToSortMergeJoin ( hint : JoinHint ) : Boolean = { hint . leftHint . exists ( _ . strategy . contains ( SHUFFLE_MERGE )) || hint . rightHint . exists ( _ . strategy . contains ( SHUFFLE_MERGE )) } private def hintToShuffleReplicateNL ( hint : JoinHint ) : Boolean = { hint . leftHint . exists ( _ . strategy . contains ( SHUFFLE_REPLICATE_NL )) || hint . rightHint . exists ( _ . strategy . contains ( SHUFFLE_REPLICATE_NL )) } def apply ( plan : LogicalPlan ) : Seq [ SparkPlan ] = plan match { // If it is an equi-join, we first look at the join hints w.r.t. the following order: // 1. broadcast hint: pick broadcast hash join if the join type is supported. If both sides // have the broadcast hints, choose the smaller side (based on stats) to broadcast. // 2. sort merge hint: pick sort merge join if join keys are sortable. // 3. shuffle hash hint: We pick shuffle hash join if the join type is supported. If both // sides have the shuffle hash hints, choose the smaller side (based on stats) as the // build side. // 4. shuffle replicate NL hint: pick cartesian product if join type is inner like. // // If there is no hint or the hints are not applicable, we follow these rules one by one: // 1. Pick broadcast hash join if one side is small enough to broadcast, and the join type // is supported. If both sides are small, choose the smaller side (based on stats) // to broadcast. // 2. Pick shuffle hash join if one side is small enough to build local hash map, and is // much smaller than the other side, and `spark.sql.join.preferSortMergeJoin` is false. // 3. Pick sort merge join if the join keys are sortable. // 4. Pick cartesian product if join type is inner like. // 5. Pick broadcast nested loop join as the final solution. It may OOM but we don't have // other choice. case ExtractEquiJoinKeys ( joinType , leftKeys , rightKeys , condition , left , right , hint ) => def createBroadcastHashJoin ( buildLeft : Boolean , buildRight : Boolean ) = { val wantToBuildLeft = canBuildLeft ( joinType ) && buildLeft val wantToBuildRight = canBuildRight ( joinType ) && buildRight getBuildSide ( wantToBuildLeft , wantToBuildRight , left , right ). map { buildSide => Seq ( joins . BroadcastHashJoinExec ( leftKeys , rightKeys , joinType , buildSide , condition , planLater ( left ), planLater ( right ))) } } def createShuffleHashJoin ( buildLeft : Boolean , buildRight : Boolean ) = { val wantToBuildLeft = canBuildLeft ( joinType ) && buildLeft val wantToBuildRight = canBuildRight ( joinType ) && buildRight getBuildSide ( wantToBuildLeft , wantToBuildRight , left , right ). map { buildSide => Seq ( joins . ShuffledHashJoinExec ( leftKeys , rightKeys , joinType , buildSide , condition , planLater ( left ), planLater ( right ))) } } def createSortMergeJoin () = { if ( RowOrdering . isOrderable ( leftKeys )) { Some ( Seq ( joins . SortMergeJoinExec ( leftKeys , rightKeys , joinType , condition , planLater ( left ), planLater ( right )))) } else { None } } def createCartesianProduct () = { if ( joinType . isInstanceOf [ InnerLike ]) { Some ( Seq ( joins . CartesianProductExec ( planLater ( left ), planLater ( right ), condition ))) } else { None } } def createJoinWithoutHint () = { createBroadcastHashJoin ( canBroadcast ( left ), canBroadcast ( right )) . orElse { if (! conf . preferSortMergeJoin ) { createShuffleHashJoin ( canBuildLocalHashMap ( left ) && muchSmaller ( left , right ), canBuildLocalHashMap ( right ) && muchSmaller ( right , left )) } else { None } } . orElse ( createSortMergeJoin ()) . orElse ( createCartesianProduct ()) . getOrElse { // This join could be very slow or OOM val buildSide = getSmallerSide ( left , right ) Seq ( joins . BroadcastNestedLoopJoinExec ( planLater ( left ), planLater ( right ), buildSide , joinType , condition )) } } createBroadcastHashJoin ( hintToBroadcastLeft ( hint ), hintToBroadcastRight ( hint )) . orElse { if ( hintToSortMergeJoin ( hint )) createSortMergeJoin () else None } . orElse ( createShuffleHashJoin ( hintToShuffleHashLeft ( hint ), hintToShuffleHashRight ( hint ))) . orElse { if ( hintToShuffleReplicateNL ( hint )) createCartesianProduct () else None } . getOrElse ( createJoinWithoutHint ()) // If it is not an equi-join, we first look at the join hints w.r.t. the following order: // 1. broadcast hint: pick broadcast nested loop join. If both sides have the broadcast // hints, choose the smaller side (based on stats) to broadcast for inner and full joins, // choose the left side for right join, and choose right side for left join. // 2. shuffle replicate NL hint: pick cartesian product if join type is inner like. // // If there is no hint or the hints are not applicable, we follow these rules one by one: // 1. Pick broadcast nested loop join if one side is small enough to broadcast. If only left // side is broadcast-able and it's left join, or only right side is broadcast-able and // it's right join, we skip this rule. If both sides are small, broadcasts the smaller // side for inner and full joins, broadcasts the left side for right join, and broadcasts // right side for left join. // 2. Pick cartesian product if join type is inner like. // 3. Pick broadcast nested loop join as the final solution. It may OOM but we don't have // other choice. It broadcasts the smaller side for inner and full joins, broadcasts the // left side for right join, and broadcasts right side for left join. case logical . Join ( left , right , joinType , condition , hint ) => val desiredBuildSide = if ( joinType . isInstanceOf [ InnerLike ] || joinType == FullOuter ) { getSmallerSide ( left , right ) } else { // For perf reasons, `BroadcastNestedLoopJoinExec` prefers to broadcast left side if // it's a right join, and broadcast right side if it's a left join. // TODO: revisit it. If left side is much smaller than the right side, it may be better // to broadcast the left side even if it's a left join. if ( canBuildLeft ( joinType )) BuildLeft else BuildRight } def createBroadcastNLJoin ( buildLeft : Boolean , buildRight : Boolean ) = { val maybeBuildSide = if ( buildLeft && buildRight ) { Some ( desiredBuildSide ) } else if ( buildLeft ) { Some ( BuildLeft ) } else if ( buildRight ) { Some ( BuildRight ) } else { None } maybeBuildSide . map { buildSide => Seq ( joins . BroadcastNestedLoopJoinExec ( planLater ( left ), planLater ( right ), buildSide , joinType , condition )) } } def createCartesianProduct () = { if ( joinType . isInstanceOf [ InnerLike ]) { Some ( Seq ( joins . CartesianProductExec ( planLater ( left ), planLater ( right ), condition ))) } else { None } } def createJoinWithoutHint () = { createBroadcastNLJoin ( canBroadcast ( left ), canBroadcast ( right )) . orElse ( createCartesianProduct ()) . getOrElse { // This join could be very slow or OOM Seq ( joins . BroadcastNestedLoopJoinExec ( planLater ( left ), planLater ( right ), desiredBuildSide , joinType , condition )) } } createBroadcastNLJoin ( hintToBroadcastLeft ( hint ), hintToBroadcastRight ( hint )) . orElse { if ( hintToShuffleReplicateNL ( hint )) createCartesianProduct () else None } . getOrElse ( createJoinWithoutHint ()) // --- Cases where this strategy does not apply --------------------------------------------- case _ => Nil } } BroadcastHashJoinExec BroadcastNestedLoopJoinExec ShuffledHashJoinExec SortMergeJoinExec Reference \u00b6","title":"Join"},{"location":"join/#join","text":"joinRelation : (joinType) JOIN right=relationPrimary joinCriteria? | NATURAL joinType JOIN right=relationPrimary ; joinType : INNER? | CROSS | LEFT OUTER? | LEFT SEMI | RIGHT OUTER? | FULL OUTER? | LEFT? ANTI ; joinCriteria : ON booleanExpression | USING identifierList ; /** * Select the proper physical plan for join based on join strategy hints, the availability of * equi-join keys and the sizes of joining relations. Below are the existing join strategies, * their characteristics and their limitations. * * - Broadcast hash join (BHJ): * Only supported for equi-joins, while the join keys do not need to be sortable. * Supported for all join types except full outer joins. * BHJ usually performs faster than the other join algorithms when the broadcast side is * small. However, broadcasting tables is a network-intensive operation and it could cause * OOM or perform badly in some cases, especially when the build/broadcast side is big. * * - Shuffle hash join: * Only supported for equi-joins, while the join keys do not need to be sortable. * Supported for all join types except full outer joins. * * - Shuffle sort merge join (SMJ): * Only supported for equi-joins and the join keys have to be sortable. * Supported for all join types. * * - Broadcast nested loop join (BNLJ): * Supports both equi-joins and non-equi-joins. * Supports all the join types, but the implementation is optimized for: * 1) broadcasting the left side in a right outer join; * 2) broadcasting the right side in a left outer, left semi, left anti or existence join; * 3) broadcasting either side in an inner-like join. * For other cases, we need to scan the data multiple times, which can be rather slow. * * - Shuffle-and-replicate nested loop join (a.k.a. cartesian product join): * Supports both equi-joins and non-equi-joins. * Supports only inner like joins. */ object JoinSelection extends Strategy with PredicateHelper { /** * Matches a plan whose output should be small enough to be used in broadcast join. */ private def canBroadcast ( plan : LogicalPlan ) : Boolean = { plan . stats . sizeInBytes >= 0 && plan . stats . sizeInBytes <= conf . autoBroadcastJoinThreshold } /** * Matches a plan whose single partition should be small enough to build a hash table. * * Note: this assume that the number of partition is fixed, requires additional work if it's * dynamic. */ private def canBuildLocalHashMap ( plan : LogicalPlan ) : Boolean = { plan . stats . sizeInBytes < conf . autoBroadcastJoinThreshold * conf . numShufflePartitions } /** * Returns whether plan a is much smaller (3X) than plan b. * * The cost to build hash map is higher than sorting, we should only build hash map on a table * that is much smaller than other one. Since we does not have the statistic for number of rows, * use the size of bytes here as estimation. */ private def muchSmaller ( a : LogicalPlan , b : LogicalPlan ) : Boolean = { a . stats . sizeInBytes * 3 <= b . stats . sizeInBytes } private def canBuildRight ( joinType : JoinType ) : Boolean = joinType match { case _: InnerLike | LeftOuter | LeftSemi | LeftAnti | _ : ExistenceJoin => true case _ => false } private def canBuildLeft ( joinType : JoinType ) : Boolean = joinType match { case _: InnerLike | RightOuter => true case _ => false } private def getBuildSide ( wantToBuildLeft : Boolean , wantToBuildRight : Boolean , left : LogicalPlan , right : LogicalPlan ) : Option [ BuildSide ] = { if ( wantToBuildLeft && wantToBuildRight ) { // returns the smaller side base on its estimated physical size, if we want to build the // both sides. Some ( getSmallerSide ( left , right )) } else if ( wantToBuildLeft ) { Some ( BuildLeft ) } else if ( wantToBuildRight ) { Some ( BuildRight ) } else { None } } private def getSmallerSide ( left : LogicalPlan , right : LogicalPlan ) = { if ( right . stats . sizeInBytes <= left . stats . sizeInBytes ) BuildRight else BuildLeft } private def hintToBroadcastLeft ( hint : JoinHint ) : Boolean = { hint . leftHint . exists ( _ . strategy . contains ( BROADCAST )) } private def hintToBroadcastRight ( hint : JoinHint ) : Boolean = { hint . rightHint . exists ( _ . strategy . contains ( BROADCAST )) } private def hintToShuffleHashLeft ( hint : JoinHint ) : Boolean = { hint . leftHint . exists ( _ . strategy . contains ( SHUFFLE_HASH )) } private def hintToShuffleHashRight ( hint : JoinHint ) : Boolean = { hint . rightHint . exists ( _ . strategy . contains ( SHUFFLE_HASH )) } private def hintToSortMergeJoin ( hint : JoinHint ) : Boolean = { hint . leftHint . exists ( _ . strategy . contains ( SHUFFLE_MERGE )) || hint . rightHint . exists ( _ . strategy . contains ( SHUFFLE_MERGE )) } private def hintToShuffleReplicateNL ( hint : JoinHint ) : Boolean = { hint . leftHint . exists ( _ . strategy . contains ( SHUFFLE_REPLICATE_NL )) || hint . rightHint . exists ( _ . strategy . contains ( SHUFFLE_REPLICATE_NL )) } def apply ( plan : LogicalPlan ) : Seq [ SparkPlan ] = plan match { // If it is an equi-join, we first look at the join hints w.r.t. the following order: // 1. broadcast hint: pick broadcast hash join if the join type is supported. If both sides // have the broadcast hints, choose the smaller side (based on stats) to broadcast. // 2. sort merge hint: pick sort merge join if join keys are sortable. // 3. shuffle hash hint: We pick shuffle hash join if the join type is supported. If both // sides have the shuffle hash hints, choose the smaller side (based on stats) as the // build side. // 4. shuffle replicate NL hint: pick cartesian product if join type is inner like. // // If there is no hint or the hints are not applicable, we follow these rules one by one: // 1. Pick broadcast hash join if one side is small enough to broadcast, and the join type // is supported. If both sides are small, choose the smaller side (based on stats) // to broadcast. // 2. Pick shuffle hash join if one side is small enough to build local hash map, and is // much smaller than the other side, and `spark.sql.join.preferSortMergeJoin` is false. // 3. Pick sort merge join if the join keys are sortable. // 4. Pick cartesian product if join type is inner like. // 5. Pick broadcast nested loop join as the final solution. It may OOM but we don't have // other choice. case ExtractEquiJoinKeys ( joinType , leftKeys , rightKeys , condition , left , right , hint ) => def createBroadcastHashJoin ( buildLeft : Boolean , buildRight : Boolean ) = { val wantToBuildLeft = canBuildLeft ( joinType ) && buildLeft val wantToBuildRight = canBuildRight ( joinType ) && buildRight getBuildSide ( wantToBuildLeft , wantToBuildRight , left , right ). map { buildSide => Seq ( joins . BroadcastHashJoinExec ( leftKeys , rightKeys , joinType , buildSide , condition , planLater ( left ), planLater ( right ))) } } def createShuffleHashJoin ( buildLeft : Boolean , buildRight : Boolean ) = { val wantToBuildLeft = canBuildLeft ( joinType ) && buildLeft val wantToBuildRight = canBuildRight ( joinType ) && buildRight getBuildSide ( wantToBuildLeft , wantToBuildRight , left , right ). map { buildSide => Seq ( joins . ShuffledHashJoinExec ( leftKeys , rightKeys , joinType , buildSide , condition , planLater ( left ), planLater ( right ))) } } def createSortMergeJoin () = { if ( RowOrdering . isOrderable ( leftKeys )) { Some ( Seq ( joins . SortMergeJoinExec ( leftKeys , rightKeys , joinType , condition , planLater ( left ), planLater ( right )))) } else { None } } def createCartesianProduct () = { if ( joinType . isInstanceOf [ InnerLike ]) { Some ( Seq ( joins . CartesianProductExec ( planLater ( left ), planLater ( right ), condition ))) } else { None } } def createJoinWithoutHint () = { createBroadcastHashJoin ( canBroadcast ( left ), canBroadcast ( right )) . orElse { if (! conf . preferSortMergeJoin ) { createShuffleHashJoin ( canBuildLocalHashMap ( left ) && muchSmaller ( left , right ), canBuildLocalHashMap ( right ) && muchSmaller ( right , left )) } else { None } } . orElse ( createSortMergeJoin ()) . orElse ( createCartesianProduct ()) . getOrElse { // This join could be very slow or OOM val buildSide = getSmallerSide ( left , right ) Seq ( joins . BroadcastNestedLoopJoinExec ( planLater ( left ), planLater ( right ), buildSide , joinType , condition )) } } createBroadcastHashJoin ( hintToBroadcastLeft ( hint ), hintToBroadcastRight ( hint )) . orElse { if ( hintToSortMergeJoin ( hint )) createSortMergeJoin () else None } . orElse ( createShuffleHashJoin ( hintToShuffleHashLeft ( hint ), hintToShuffleHashRight ( hint ))) . orElse { if ( hintToShuffleReplicateNL ( hint )) createCartesianProduct () else None } . getOrElse ( createJoinWithoutHint ()) // If it is not an equi-join, we first look at the join hints w.r.t. the following order: // 1. broadcast hint: pick broadcast nested loop join. If both sides have the broadcast // hints, choose the smaller side (based on stats) to broadcast for inner and full joins, // choose the left side for right join, and choose right side for left join. // 2. shuffle replicate NL hint: pick cartesian product if join type is inner like. // // If there is no hint or the hints are not applicable, we follow these rules one by one: // 1. Pick broadcast nested loop join if one side is small enough to broadcast. If only left // side is broadcast-able and it's left join, or only right side is broadcast-able and // it's right join, we skip this rule. If both sides are small, broadcasts the smaller // side for inner and full joins, broadcasts the left side for right join, and broadcasts // right side for left join. // 2. Pick cartesian product if join type is inner like. // 3. Pick broadcast nested loop join as the final solution. It may OOM but we don't have // other choice. It broadcasts the smaller side for inner and full joins, broadcasts the // left side for right join, and broadcasts right side for left join. case logical . Join ( left , right , joinType , condition , hint ) => val desiredBuildSide = if ( joinType . isInstanceOf [ InnerLike ] || joinType == FullOuter ) { getSmallerSide ( left , right ) } else { // For perf reasons, `BroadcastNestedLoopJoinExec` prefers to broadcast left side if // it's a right join, and broadcast right side if it's a left join. // TODO: revisit it. If left side is much smaller than the right side, it may be better // to broadcast the left side even if it's a left join. if ( canBuildLeft ( joinType )) BuildLeft else BuildRight } def createBroadcastNLJoin ( buildLeft : Boolean , buildRight : Boolean ) = { val maybeBuildSide = if ( buildLeft && buildRight ) { Some ( desiredBuildSide ) } else if ( buildLeft ) { Some ( BuildLeft ) } else if ( buildRight ) { Some ( BuildRight ) } else { None } maybeBuildSide . map { buildSide => Seq ( joins . BroadcastNestedLoopJoinExec ( planLater ( left ), planLater ( right ), buildSide , joinType , condition )) } } def createCartesianProduct () = { if ( joinType . isInstanceOf [ InnerLike ]) { Some ( Seq ( joins . CartesianProductExec ( planLater ( left ), planLater ( right ), condition ))) } else { None } } def createJoinWithoutHint () = { createBroadcastNLJoin ( canBroadcast ( left ), canBroadcast ( right )) . orElse ( createCartesianProduct ()) . getOrElse { // This join could be very slow or OOM Seq ( joins . BroadcastNestedLoopJoinExec ( planLater ( left ), planLater ( right ), desiredBuildSide , joinType , condition )) } } createBroadcastNLJoin ( hintToBroadcastLeft ( hint ), hintToBroadcastRight ( hint )) . orElse { if ( hintToShuffleReplicateNL ( hint )) createCartesianProduct () else None } . getOrElse ( createJoinWithoutHint ()) // --- Cases where this strategy does not apply --------------------------------------------- case _ => Nil } } BroadcastHashJoinExec BroadcastNestedLoopJoinExec ShuffledHashJoinExec SortMergeJoinExec","title":"Join"},{"location":"join/#reference","text":"","title":"Reference"},{"location":"lateralview/","text":"Lateral View \u00b6 lateralView : LATERAL VIEW (OUTER)? qualifiedName '(' (expression (',' expression)*)? ')' tblName=identifier (AS? colName+=identifier (',' colName+=identifier)*)? ; Reference \u00b6","title":"Lateral View"},{"location":"lateralview/#lateral-view","text":"lateralView : LATERAL VIEW (OUTER)? qualifiedName '(' (expression (',' expression)*)? ')' tblName=identifier (AS? colName+=identifier (',' colName+=identifier)*)? ;","title":"Lateral View"},{"location":"lateralview/#reference","text":"","title":"Reference"},{"location":"mllib/","text":"MLlib \u00b6 Reference \u00b6 Machine Learning Library (MLlib) Guide","title":"MLlib"},{"location":"mllib/#mllib","text":"","title":"MLlib"},{"location":"mllib/#reference","text":"Machine Learning Library (MLlib) Guide","title":"Reference"},{"location":"olap/","text":"OLAP \u00b6 aggregation : GROUP BY groupingExpressions+=expression (',' groupingExpressions+=expression)* ( WITH kind=ROLLUP | WITH kind=CUBE | kind=GROUPING SETS '(' groupingSet (',' groupingSet)* ')')? | GROUP BY kind=GROUPING SETS '(' groupingSet (',' groupingSet)* ')' ; groupingSet : '(' (expression (',' expression)*)? ')' | expression ; GROUPING SETS/ROLLUP/CUBE \u00b6 Parsed Logical Plan -> Expand(mapPartitions)\u3001Aggregate(Hash#mapPartitionsWithIndex) \u5176\u4e2d ROLLUP / CUBE \u4e3a GROUPING SETS \u7684\u7279\u4f8b ROLLUP \u7528\u4e8e\u652f\u6301\u4e0a\u5377\u548c\u4e0b\u94bb\u7684\u60c5\u51b5\u7684\u7b80\u6613\u5199\u6cd5 CUBE \u7528\u4e8e\u652f\u6301\u6784\u5efa\u7acb\u65b9\u4f53\u7684\u60c5\u51b5\u7684\u7b80\u6613\u5199\u6cd5 Reference \u00b6","title":"OLAP"},{"location":"olap/#olap","text":"aggregation : GROUP BY groupingExpressions+=expression (',' groupingExpressions+=expression)* ( WITH kind=ROLLUP | WITH kind=CUBE | kind=GROUPING SETS '(' groupingSet (',' groupingSet)* ')')? | GROUP BY kind=GROUPING SETS '(' groupingSet (',' groupingSet)* ')' ; groupingSet : '(' (expression (',' expression)*)? ')' | expression ;","title":"OLAP"},{"location":"olap/#grouping-setsrollupcube","text":"Parsed Logical Plan -> Expand(mapPartitions)\u3001Aggregate(Hash#mapPartitionsWithIndex) \u5176\u4e2d ROLLUP / CUBE \u4e3a GROUPING SETS \u7684\u7279\u4f8b ROLLUP \u7528\u4e8e\u652f\u6301\u4e0a\u5377\u548c\u4e0b\u94bb\u7684\u60c5\u51b5\u7684\u7b80\u6613\u5199\u6cd5 CUBE \u7528\u4e8e\u652f\u6301\u6784\u5efa\u7acb\u65b9\u4f53\u7684\u60c5\u51b5\u7684\u7b80\u6613\u5199\u6cd5","title":"GROUPING SETS/ROLLUP/CUBE"},{"location":"olap/#reference","text":"","title":"Reference"},{"location":"rdd/","text":"RDD \u00b6 Characteristics \u00b6 Partitions PreferredLocations Dependencies Iterator Partitioner Operations \u00b6 Creation Operation Transformation Operation Storage Operation Action Operation Dependencies \u00b6 NarrowDependency OneToOneDependency RangeDependency ShuffleDependency Stage \u00b6 ResultStage ShuffleMapStage Reference \u00b6 RDD Programming Guide","title":"RDD"},{"location":"rdd/#rdd","text":"","title":"RDD"},{"location":"rdd/#characteristics","text":"Partitions PreferredLocations Dependencies Iterator Partitioner","title":"Characteristics"},{"location":"rdd/#operations","text":"Creation Operation Transformation Operation Storage Operation Action Operation","title":"Operations"},{"location":"rdd/#dependencies","text":"NarrowDependency OneToOneDependency RangeDependency ShuffleDependency","title":"Dependencies"},{"location":"rdd/#stage","text":"ResultStage ShuffleMapStage","title":"Stage"},{"location":"rdd/#reference","text":"RDD Programming Guide","title":"Reference"},{"location":"reshape/","text":"Reshape \u00b6 pivotClause : PIVOT '(' aggregates=namedExpressionSeq FOR pivotColumn IN '(' pivotValues+=pivotValue (',' pivotValues+=pivotValue)* ')' ')' ; pivotColumn : identifiers+=identifier | '(' identifiers+=identifier (',' identifiers+=identifier)* ')' ; pivotValue : expression (AS? identifier)? ; Reference \u00b6","title":"Reshape"},{"location":"reshape/#reshape","text":"pivotClause : PIVOT '(' aggregates=namedExpressionSeq FOR pivotColumn IN '(' pivotValues+=pivotValue (',' pivotValues+=pivotValue)* ')' ')' ; pivotColumn : identifiers+=identifier | '(' identifiers+=identifier (',' identifiers+=identifier)* ')' ; pivotValue : expression (AS? identifier)? ;","title":"Reshape"},{"location":"reshape/#reference","text":"","title":"Reference"},{"location":"schedule/","text":"Schedule \u00b6 Reference \u00b6","title":"Schedule"},{"location":"schedule/#schedule","text":"","title":"Schedule"},{"location":"schedule/#reference","text":"","title":"Reference"},{"location":"sql/","text":"SQL \u00b6 \u5b58\u50a8\u3001\u8ba1\u7b97 \u7d22\u5f15\u3001\u7f13\u5b58 \u6d17\u724c\u3001\u503e\u659c Reference \u00b6 Spark SQL, DataFrames and Datasets Guide Spark SQL, Built-in Functions SQL Guide DataFrames and Datasets","title":"SQL"},{"location":"sql/#sql","text":"\u5b58\u50a8\u3001\u8ba1\u7b97 \u7d22\u5f15\u3001\u7f13\u5b58 \u6d17\u724c\u3001\u503e\u659c","title":"SQL"},{"location":"sql/#reference","text":"Spark SQL, DataFrames and Datasets Guide Spark SQL, Built-in Functions SQL Guide DataFrames and Datasets","title":"Reference"},{"location":"statistics/","text":"Statistics \u00b6 Estimates of various statistics. The default estimation logic simply lazily multiplies the corresponding statistic produced by the children. Statistics -> CatalogStatistics \u00b6 sizeInBytes: Physical size in bytes. For leaf operators this defaults to 1, otherwise it defaults to the product of children's sizeInBytes rowCount: Estimated number of rows attributeStats: Statistics for Attributes hints: Query hints ColumnStat -> CatalogColumnStat \u00b6 distinctCount: number of distinct values min: minimum value max: maximum value nullCount: number of nulls avgLen: average length of the values maxLen: maximum length of the values histogram: histogram of the values Histogram[HistogramBin] \u00b6 height: number of rows in each bin bins: equi-height histogram bins lo: lower bound of the value range in this bin hi: higher bound of the value range in this bin ndv: approximate number of distinct values in this bin HintInfo \u00b6 broadcast join/shuffle DataFrameStatFunctions \u00b6 Statistic functions for DataFrames.(Since: 1.4.0) approxQuantile: Calculates the approximate quantiles of numerical columns of a DataFrame bloomFilter: Builds a Bloom filter over a specified column corr: Calculates the Pearson Correlation Coefficient of two columns of a DataFrame countMinSketch: Builds a Count-min Sketch over a specified column cov: Calculate the sample covariance of two numerical columns of a DataFrame crosstab: Computes a pair-wise frequency table of the given columns freqItems: (Scala-specific) Finding frequent items for columns, possibly with false positives sampleBy: Returns a stratified sample without replacement based on the fraction given on each stratum Other \u00b6 Dataset#describe \u00b6 count, mean, stddev, min, max StatFunctions.summary(ds, Seq(\"count\", \"mean\", \"stddev\", \"min\", \"25%\", \"50%\", \"75%\", \"max\")) Statistics \u00b6 API for statistical functions in MLlib colStats[MultivariateOnlineSummarizer]: column-wise summary statistics corr: Pearson correlation matrix chiSqTest: chi-squared test kolmogorovSmirnovTest: Kolmogorov-Smirnov test Reference \u00b6 Spark SQL Package Summary","title":"Statistics"},{"location":"statistics/#statistics","text":"Estimates of various statistics. The default estimation logic simply lazily multiplies the corresponding statistic produced by the children.","title":"Statistics"},{"location":"statistics/#statistics-catalogstatistics","text":"sizeInBytes: Physical size in bytes. For leaf operators this defaults to 1, otherwise it defaults to the product of children's sizeInBytes rowCount: Estimated number of rows attributeStats: Statistics for Attributes hints: Query hints","title":"Statistics -&gt; CatalogStatistics"},{"location":"statistics/#columnstat-catalogcolumnstat","text":"distinctCount: number of distinct values min: minimum value max: maximum value nullCount: number of nulls avgLen: average length of the values maxLen: maximum length of the values histogram: histogram of the values","title":"ColumnStat -&gt; CatalogColumnStat"},{"location":"statistics/#histogramhistogrambin","text":"height: number of rows in each bin bins: equi-height histogram bins lo: lower bound of the value range in this bin hi: higher bound of the value range in this bin ndv: approximate number of distinct values in this bin","title":"Histogram[HistogramBin]"},{"location":"statistics/#hintinfo","text":"broadcast join/shuffle","title":"HintInfo"},{"location":"statistics/#dataframestatfunctions","text":"Statistic functions for DataFrames.(Since: 1.4.0) approxQuantile: Calculates the approximate quantiles of numerical columns of a DataFrame bloomFilter: Builds a Bloom filter over a specified column corr: Calculates the Pearson Correlation Coefficient of two columns of a DataFrame countMinSketch: Builds a Count-min Sketch over a specified column cov: Calculate the sample covariance of two numerical columns of a DataFrame crosstab: Computes a pair-wise frequency table of the given columns freqItems: (Scala-specific) Finding frequent items for columns, possibly with false positives sampleBy: Returns a stratified sample without replacement based on the fraction given on each stratum","title":"DataFrameStatFunctions"},{"location":"statistics/#other","text":"","title":"Other"},{"location":"statistics/#datasetdescribe","text":"count, mean, stddev, min, max StatFunctions.summary(ds, Seq(\"count\", \"mean\", \"stddev\", \"min\", \"25%\", \"50%\", \"75%\", \"max\"))","title":"Dataset#describe"},{"location":"statistics/#statistics_1","text":"API for statistical functions in MLlib colStats[MultivariateOnlineSummarizer]: column-wise summary statistics corr: Pearson correlation matrix chiSqTest: chi-squared test kolmogorovSmirnovTest: Kolmogorov-Smirnov test","title":"Statistics"},{"location":"statistics/#reference","text":"Spark SQL Package Summary","title":"Reference"},{"location":"storage/","text":"","title":"Storage"},{"location":"streaming/","text":"Streaming \u00b6 Reference \u00b6 Structured Streaming Programming Guide Spark Streaming Programming Guide lw-lin/CoolplaySpark","title":"Streaming"},{"location":"streaming/#streaming","text":"","title":"Streaming"},{"location":"streaming/#reference","text":"Structured Streaming Programming Guide Spark Streaming Programming Guide lw-lin/CoolplaySpark","title":"Reference"},{"location":"structured-streaming/","text":"Structured Streaming \u00b6 Reference \u00b6 Structured Streaming Programming Guide Structured Streaming Guide lw-lin/CoolplaySpark Apache Spark Structured Streaming","title":"Structured Streaming"},{"location":"structured-streaming/#structured-streaming","text":"","title":"Structured Streaming"},{"location":"structured-streaming/#reference","text":"Structured Streaming Programming Guide Structured Streaming Guide lw-lin/CoolplaySpark Apache Spark Structured Streaming","title":"Reference"},{"location":"todo/","text":"TODO \u00b6 Distinct/Group by","title":"TODO"},{"location":"todo/#todo","text":"Distinct/Group by","title":"TODO"},{"location":"window/","text":"Window \u00b6 windowSpec : name=identifier #windowRef | '('name=identifier')' #windowRef | '(' ( CLUSTER BY partition+=expression (',' partition+=expression)* | ((PARTITION | DISTRIBUTE) BY partition+=expression (',' partition+=expression)*)? ((ORDER | SORT) BY sortItem (',' sortItem)*)?) windowFrame? ')' #windowDef ; windowFrame : frameType=RANGE start=frameBound | frameType=ROWS start=frameBound | frameType=RANGE BETWEEN start=frameBound AND end=frameBound | frameType=ROWS BETWEEN start=frameBound AND end=frameBound ; frameBound : UNBOUNDED boundType=(PRECEDING | FOLLOWING) | boundType=CURRENT ROW | expression boundType=(PRECEDING | FOLLOWING) ; Window functions \u00b6 cume_dist dense_rank lag lead ntile percent_rank rank row_number Reference \u00b6","title":"Window"},{"location":"window/#window","text":"windowSpec : name=identifier #windowRef | '('name=identifier')' #windowRef | '(' ( CLUSTER BY partition+=expression (',' partition+=expression)* | ((PARTITION | DISTRIBUTE) BY partition+=expression (',' partition+=expression)*)? ((ORDER | SORT) BY sortItem (',' sortItem)*)?) windowFrame? ')' #windowDef ; windowFrame : frameType=RANGE start=frameBound | frameType=ROWS start=frameBound | frameType=RANGE BETWEEN start=frameBound AND end=frameBound | frameType=ROWS BETWEEN start=frameBound AND end=frameBound ; frameBound : UNBOUNDED boundType=(PRECEDING | FOLLOWING) | boundType=CURRENT ROW | expression boundType=(PRECEDING | FOLLOWING) ;","title":"Window"},{"location":"window/#window-functions","text":"cume_dist dense_rank lag lead ntile percent_rank rank row_number","title":"Window functions"},{"location":"window/#reference","text":"","title":"Reference"}]}